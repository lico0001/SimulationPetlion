{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db0bd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-wzr/.conda/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.append(\"/home/jupyter-wzr/.local/lib/python3.8/site-packages\")\n",
    "import warnings\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# 其实线性回归就是一个不加激活函数的全链接层\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import glob,os\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # 设置可⻅GPU，从0开始"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9509dc0",
   "metadata": {},
   "source": [
    "# 数据读取与处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de52fce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "874668"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames= []\n",
    "for m in range (10):\n",
    "    for n in range(m+1,11):        \n",
    "        strm = 'soc_start_'+str(m)\n",
    "        strn = 'soc_end_'+str(n)\n",
    "        path=r'/home/jupyter-wzr/0927/0929data/'+strm+'/'+strn\n",
    "        filename=glob.glob(os.path.join(path, \"*.csv\"))\n",
    "        filenames.extend(filename)\n",
    "\n",
    "dl = []\n",
    "for f in filenames:\n",
    "    dl.append(pd.read_csv(f, header=0, sep=','))\n",
    "df = pd.concat(dl).loc[:, ['set_P','SOC_start','SOC_end','set_SOH','delta_SOH']]\n",
    "df.iloc[:,0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7109d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set_P</th>\n",
       "      <th>SOC_start</th>\n",
       "      <th>SOC_end</th>\n",
       "      <th>set_SOH</th>\n",
       "      <th>delta_SOH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>-1.397442</td>\n",
       "      <td>-0.175789</td>\n",
       "      <td>-0.340301</td>\n",
       "      <td>1.151305</td>\n",
       "      <td>0.262872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>1.016128</td>\n",
       "      <td>-0.628658</td>\n",
       "      <td>-0.645394</td>\n",
       "      <td>-0.040953</td>\n",
       "      <td>0.493682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16332</th>\n",
       "      <td>-0.432014</td>\n",
       "      <td>-0.830430</td>\n",
       "      <td>-0.951360</td>\n",
       "      <td>-1.512175</td>\n",
       "      <td>0.584586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.211604</td>\n",
       "      <td>0.828839</td>\n",
       "      <td>0.576933</td>\n",
       "      <td>1.382981</td>\n",
       "      <td>-0.162997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>1.820651</td>\n",
       "      <td>2.168491</td>\n",
       "      <td>1.681589</td>\n",
       "      <td>0.548350</td>\n",
       "      <td>0.424210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>1.659746</td>\n",
       "      <td>-0.810741</td>\n",
       "      <td>-0.950812</td>\n",
       "      <td>1.539562</td>\n",
       "      <td>0.573648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17943</th>\n",
       "      <td>-1.236538</td>\n",
       "      <td>-0.774468</td>\n",
       "      <td>-0.951492</td>\n",
       "      <td>-0.048443</td>\n",
       "      <td>0.600045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.372509</td>\n",
       "      <td>1.163521</td>\n",
       "      <td>1.491877</td>\n",
       "      <td>1.851919</td>\n",
       "      <td>-1.705633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>1.177032</td>\n",
       "      <td>-0.512019</td>\n",
       "      <td>-0.645339</td>\n",
       "      <td>-0.310336</td>\n",
       "      <td>0.460097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47559</th>\n",
       "      <td>-1.236538</td>\n",
       "      <td>-0.737071</td>\n",
       "      <td>-0.951477</td>\n",
       "      <td>-1.456587</td>\n",
       "      <td>0.604934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          set_P  SOC_start   SOC_end   set_SOH  delta_SOH\n",
       "282   -1.397442  -0.175789 -0.340301  1.151305   0.262872\n",
       "1651   1.016128  -0.628658 -0.645394 -0.040953   0.493682\n",
       "16332 -0.432014  -0.830430 -0.951360 -1.512175   0.584586\n",
       "95     0.211604   0.828839  0.576933  1.382981  -0.162997\n",
       "949    1.820651   2.168491  1.681589  0.548350   0.424210\n",
       "1016   1.659746  -0.810741 -0.950812  1.539562   0.573648\n",
       "17943 -1.236538  -0.774468 -0.951492 -0.048443   0.600045\n",
       "3      0.372509   1.163521  1.491877  1.851919  -1.705633\n",
       "1647   1.177032  -0.512019 -0.645339 -0.310336   0.460097\n",
       "47559 -1.236538  -0.737071 -0.951477 -1.456587   0.604934"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zscores = preprocessing.StandardScaler()\n",
    "# zscore标准化\n",
    "zscore = zscores.fit_transform(df)\n",
    "df_zscore = pd.DataFrame(zscore,index=df.index,columns = df.columns)#df.columns\n",
    "df_comp = sklearn.utils.shuffle(df_zscore)\n",
    "df_comp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63fee562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择变量\n",
    "y=df_comp['delta_SOH']\n",
    "x=df_comp[['set_P','SOC_start','SOC_end','set_SOH']]\n",
    "\n",
    "# 切分训练集与测试集与验证集\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(x,y, test_size=0.2)  # 不要设置随机种子random_state保证每次划分的随机性\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y, test_size=0.5)  # 不要设置随机种子random_state保证每次划分的随机性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017d29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = np.array(x_train), np.array(x_val), np.array(y_train), np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1334ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = \\\n",
    "torch.tensor(x_train), torch.tensor(x_val), torch.tensor(y_train), torch.tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b10b0a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437334, 437334)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train),len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45dc3bd",
   "metadata": {},
   "source": [
    "# ICNN模型建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3461acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CvxModel(nn.Module):\n",
    "    '''\n",
    "        输入凸神经网络结构定义\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_feature=4, n_hidden=4, n_output=1, nGdIter=30):\n",
    "        super(CvxModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(n_feature, n_hidden, bias=False)\n",
    "        self.hidden_layer = nn.Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.output_layer = nn.Linear(n_hidden, n_output, bias=False)\n",
    "        self.passthrough_layer = nn.Linear(n_feature, n_hidden)\n",
    "        self.passthrough_output_layer = nn.Linear(n_feature, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        zx1 = F.relu(self.input_layer(x))\n",
    "        pass1 = self.passthrough_layer(x)\n",
    "        pass2 = self.passthrough_output_layer(x)\n",
    "        zx2 = F.relu(self.hidden_layer(zx1) + pass1)\n",
    "        zx3 = self.output_layer(zx2) + pass2\n",
    "        zx3 = zx3.squeeze(-1)\n",
    "        return zx3\n",
    "\n",
    "\n",
    "class weightConstraint():\n",
    "    '''\n",
    "        对一个特定的层的weight进行参数限制\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, module):\n",
    "        if hasattr(module, 'weight'):\n",
    "            w = module.weight.data\n",
    "            w = w.clamp(0.0, 100.0) #clamp函数可以设置最大最小值，\n",
    "            module.weight.data = w\n",
    "\n",
    "\n",
    "# 当对权重有非负的限制时，训练神经网络\n",
    "def train_with_constrains(): \n",
    "    model = CvxModel(n_feature=4, n_hidden=200, n_output=1)\n",
    "    constraints = weightConstraint()\n",
    "    criterion = torch.nn.MSELoss(reduction='sum')\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    terminate_threshold = 1e-3 # 1e-5\n",
    "    i = 0\n",
    "    print(\"当前程序开始运行时间为：{}\".format(time.asctime(time.localtime(time.time()))))\n",
    "    while (True):\n",
    "        \n",
    "        y_pred = model(x_train)\n",
    "        loss = (criterion((y_pred).float(), (y_train).float()) / y_train.size()[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model._modules['input_layer'].apply(constraints)\n",
    "        model._modules['hidden_layer'].apply(constraints)\n",
    "        model._modules['output_layer'].apply(constraints)\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            print(\"当前程序运行结束时间为：{}\".format(time.asctime(time.localtime(time.time()))))\n",
    "            print(\"当前正在训练第{}次， 损失是{}\".format(i, loss.item()))\n",
    "#             print(y_train-y_pred)\n",
    "            # test_model(model)\n",
    "            # weight_print(model)\n",
    "        if loss.item() < terminate_threshold:\n",
    "            print(\"训练满足精度要求，训练了{}次。\".format(i))\n",
    "            break\n",
    "\n",
    "def test_model(model):\n",
    "    print(model.parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f1dffbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前程序开始运行时间为：Thu Oct 13 10:28:51 2022\n",
      "当前程序运行结束时间为：Thu Oct 13 10:30:10 2022\n",
      "当前正在训练第100次， 损失是1.5153485536575317\n",
      "当前程序运行结束时间为：Thu Oct 13 10:31:27 2022\n",
      "当前正在训练第200次， 损失是1.1870265007019043\n",
      "当前程序运行结束时间为：Thu Oct 13 10:32:46 2022\n",
      "当前正在训练第300次， 损失是0.9968810081481934\n",
      "当前程序运行结束时间为：Thu Oct 13 10:34:05 2022\n",
      "当前正在训练第400次， 损失是0.859103798866272\n",
      "当前程序运行结束时间为：Thu Oct 13 10:35:25 2022\n",
      "当前正在训练第500次， 损失是0.759321391582489\n",
      "当前程序运行结束时间为：Thu Oct 13 10:36:43 2022\n",
      "当前正在训练第600次， 损失是0.6864757537841797\n",
      "当前程序运行结束时间为：Thu Oct 13 10:37:59 2022\n",
      "当前正在训练第700次， 损失是0.6311678886413574\n",
      "当前程序运行结束时间为：Thu Oct 13 10:39:17 2022\n",
      "当前正在训练第800次， 损失是0.5848026871681213\n",
      "当前程序运行结束时间为：Thu Oct 13 10:40:37 2022\n",
      "当前正在训练第900次， 损失是0.5387942790985107\n",
      "当前程序运行结束时间为：Thu Oct 13 10:42:00 2022\n",
      "当前正在训练第1000次， 损失是0.47864997386932373\n",
      "当前程序运行结束时间为：Thu Oct 13 10:43:20 2022\n",
      "当前正在训练第1100次， 损失是0.4168975353240967\n",
      "当前程序运行结束时间为：Thu Oct 13 10:44:37 2022\n",
      "当前正在训练第1200次， 损失是0.367170125246048\n",
      "当前程序运行结束时间为：Thu Oct 13 10:45:52 2022\n",
      "当前正在训练第1300次， 损失是0.32502636313438416\n",
      "当前程序运行结束时间为：Thu Oct 13 10:47:10 2022\n",
      "当前正在训练第1400次， 损失是0.2883181571960449\n",
      "当前程序运行结束时间为：Thu Oct 13 10:48:28 2022\n",
      "当前正在训练第1500次， 损失是0.2545258104801178\n",
      "当前程序运行结束时间为：Thu Oct 13 10:49:46 2022\n",
      "当前正在训练第1600次， 损失是0.22633223235607147\n",
      "当前程序运行结束时间为：Thu Oct 13 10:51:03 2022\n",
      "当前正在训练第1700次， 损失是0.20205505192279816\n",
      "当前程序运行结束时间为：Thu Oct 13 10:52:22 2022\n",
      "当前正在训练第1800次， 损失是0.18082289397716522\n",
      "当前程序运行结束时间为：Thu Oct 13 10:53:38 2022\n",
      "当前正在训练第1900次， 损失是0.16225728392601013\n",
      "当前程序运行结束时间为：Thu Oct 13 10:54:54 2022\n",
      "当前正在训练第2000次， 损失是0.14603354036808014\n",
      "当前程序运行结束时间为：Thu Oct 13 10:56:12 2022\n",
      "当前正在训练第2100次， 损失是0.131828173995018\n",
      "当前程序运行结束时间为：Thu Oct 13 10:57:31 2022\n",
      "当前正在训练第2200次， 损失是0.11950242519378662\n",
      "当前程序运行结束时间为：Thu Oct 13 10:58:48 2022\n",
      "当前正在训练第2300次， 损失是0.10879595577716827\n",
      "当前程序运行结束时间为：Thu Oct 13 11:00:05 2022\n",
      "当前正在训练第2400次， 损失是0.09957563877105713\n",
      "当前程序运行结束时间为：Thu Oct 13 11:01:23 2022\n",
      "当前正在训练第2500次， 损失是0.09160729497671127\n",
      "当前程序运行结束时间为：Thu Oct 13 11:02:39 2022\n",
      "当前正在训练第2600次， 损失是0.08470229804515839\n",
      "当前程序运行结束时间为：Thu Oct 13 11:03:56 2022\n",
      "当前正在训练第2700次， 损失是0.0787324383854866\n",
      "当前程序运行结束时间为：Thu Oct 13 11:05:15 2022\n",
      "当前正在训练第2800次， 损失是0.07357767969369888\n",
      "当前程序运行结束时间为：Thu Oct 13 11:06:31 2022\n",
      "当前正在训练第2900次， 损失是0.06909454613924026\n",
      "当前程序运行结束时间为：Thu Oct 13 11:07:47 2022\n",
      "当前正在训练第3000次， 损失是0.06518393009901047\n",
      "当前程序运行结束时间为：Thu Oct 13 11:09:03 2022\n",
      "当前正在训练第3100次， 损失是0.06177685037255287\n",
      "当前程序运行结束时间为：Thu Oct 13 11:10:19 2022\n",
      "当前正在训练第3200次， 损失是0.05884682759642601\n",
      "当前程序运行结束时间为：Thu Oct 13 11:11:36 2022\n",
      "当前正在训练第3300次， 损失是0.05635744705796242\n",
      "当前程序运行结束时间为：Thu Oct 13 11:12:51 2022\n",
      "当前正在训练第3400次， 损失是0.05423000827431679\n",
      "当前程序运行结束时间为：Thu Oct 13 11:14:08 2022\n",
      "当前正在训练第3500次， 损失是0.052426449954509735\n",
      "当前程序运行结束时间为：Thu Oct 13 11:15:25 2022\n",
      "当前正在训练第3600次， 损失是0.05091388523578644\n",
      "当前程序运行结束时间为：Thu Oct 13 11:16:42 2022\n",
      "当前正在训练第3700次， 损失是0.049637842923402786\n",
      "当前程序运行结束时间为：Thu Oct 13 11:17:59 2022\n",
      "当前正在训练第3800次， 损失是0.04855729639530182\n",
      "当前程序运行结束时间为：Thu Oct 13 11:19:16 2022\n",
      "当前正在训练第3900次， 损失是0.047642357647418976\n",
      "当前程序运行结束时间为：Thu Oct 13 11:20:31 2022\n",
      "当前正在训练第4000次， 损失是0.04687400534749031\n",
      "当前程序运行结束时间为：Thu Oct 13 11:21:48 2022\n",
      "当前正在训练第4100次， 损失是0.046230100095272064\n",
      "当前程序运行结束时间为：Thu Oct 13 11:23:06 2022\n",
      "当前正在训练第4200次， 损失是0.0456974133849144\n",
      "当前程序运行结束时间为：Thu Oct 13 11:24:27 2022\n",
      "当前正在训练第4300次， 损失是0.04524298757314682\n",
      "当前程序运行结束时间为：Thu Oct 13 11:25:49 2022\n",
      "当前正在训练第4400次， 损失是0.04481049254536629\n",
      "当前程序运行结束时间为：Thu Oct 13 11:27:07 2022\n",
      "当前正在训练第4500次， 损失是0.04443780705332756\n",
      "当前程序运行结束时间为：Thu Oct 13 11:28:27 2022\n",
      "当前正在训练第4600次， 损失是0.044135741889476776\n",
      "当前程序运行结束时间为：Thu Oct 13 11:29:46 2022\n",
      "当前正在训练第4700次， 损失是0.0438929907977581\n",
      "当前程序运行结束时间为：Thu Oct 13 11:31:04 2022\n",
      "当前正在训练第4800次， 损失是0.04370051249861717\n",
      "当前程序运行结束时间为：Thu Oct 13 11:32:22 2022\n",
      "当前正在训练第4900次， 损失是0.043548207730054855\n",
      "当前程序运行结束时间为：Thu Oct 13 11:33:40 2022\n",
      "当前正在训练第5000次， 损失是0.04342391714453697\n",
      "当前程序运行结束时间为：Thu Oct 13 11:35:00 2022\n",
      "当前正在训练第5100次， 损失是0.04332025721669197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_with_constrains()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db854753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前程序开始运行时间为：Thu Oct 13 11:36:17 2022\n",
      "当前程序运行结束时间为：Thu Oct 13 11:37:35 2022\n",
      "当前正在训练第100次， 损失是0.09338926523923874\n",
      "当前程序运行结束时间为：Thu Oct 13 11:38:55 2022\n",
      "当前正在训练第200次， 损失是0.055986445397138596\n",
      "当前程序运行结束时间为：Thu Oct 13 11:40:12 2022\n",
      "当前正在训练第300次， 损失是0.047510839998722076\n",
      "当前程序运行结束时间为：Thu Oct 13 11:41:28 2022\n",
      "当前正在训练第400次， 损失是0.046201739460229874\n",
      "当前程序运行结束时间为：Thu Oct 13 11:42:43 2022\n",
      "当前正在训练第500次， 损失是0.045715413987636566\n",
      "当前程序运行结束时间为：Thu Oct 13 11:44:00 2022\n",
      "当前正在训练第600次， 损失是0.0449470579624176\n",
      "当前程序运行结束时间为：Thu Oct 13 11:45:16 2022\n",
      "当前正在训练第700次， 损失是0.0439370721578598\n",
      "当前程序运行结束时间为：Thu Oct 13 11:46:32 2022\n",
      "当前正在训练第800次， 损失是0.043569304049015045\n",
      "当前程序运行结束时间为：Thu Oct 13 11:47:49 2022\n",
      "当前正在训练第900次， 损失是0.043525855988264084\n",
      "当前程序运行结束时间为：Thu Oct 13 11:49:05 2022\n",
      "当前正在训练第1000次， 损失是0.04318387061357498\n",
      "当前程序运行结束时间为：Thu Oct 13 11:50:22 2022\n",
      "当前正在训练第1100次， 损失是0.04329902306199074\n",
      "当前程序运行结束时间为：Thu Oct 13 11:51:41 2022\n",
      "当前正在训练第1200次， 损失是0.043030403554439545\n",
      "当前程序运行结束时间为：Thu Oct 13 11:52:59 2022\n",
      "当前正在训练第1300次， 损失是0.043963655829429626\n",
      "当前程序运行结束时间为：Thu Oct 13 11:55:34 2022\n",
      "当前正在训练第1500次， 损失是0.042996205389499664\n",
      "当前程序运行结束时间为：Thu Oct 13 11:56:49 2022\n",
      "当前正在训练第1600次， 损失是0.04299134388566017\n",
      "当前程序运行结束时间为：Thu Oct 13 11:58:07 2022\n",
      "当前正在训练第1700次， 损失是0.042990658432245255\n",
      "当前程序运行结束时间为：Thu Oct 13 11:59:24 2022\n",
      "当前正在训练第1800次， 损失是0.04353261739015579\n",
      "当前程序运行结束时间为：Thu Oct 13 12:00:43 2022\n",
      "当前正在训练第1900次， 损失是0.042926788330078125\n",
      "当前程序运行结束时间为：Thu Oct 13 12:02:00 2022\n",
      "当前正在训练第2000次， 损失是0.04291054606437683\n",
      "当前程序运行结束时间为：Thu Oct 13 12:03:17 2022\n",
      "当前正在训练第2100次， 损失是0.04290308803319931\n",
      "当前程序运行结束时间为：Thu Oct 13 12:04:36 2022\n",
      "当前正在训练第2200次， 损失是0.0428926907479763\n",
      "当前程序运行结束时间为：Thu Oct 13 12:05:52 2022\n",
      "当前正在训练第2300次， 损失是0.04288802295923233\n",
      "当前程序运行结束时间为：Thu Oct 13 12:07:10 2022\n",
      "当前正在训练第2400次， 损失是0.04288465902209282\n",
      "当前程序运行结束时间为：Thu Oct 13 12:08:28 2022\n",
      "当前正在训练第2500次， 损失是0.04288007318973541\n",
      "当前程序运行结束时间为：Thu Oct 13 12:09:46 2022\n",
      "当前正在训练第2600次， 损失是0.0429006963968277\n",
      "当前程序运行结束时间为：Thu Oct 13 12:11:04 2022\n",
      "当前正在训练第2700次， 损失是0.042874082922935486\n",
      "当前程序运行结束时间为：Thu Oct 13 12:12:22 2022\n",
      "当前正在训练第2800次， 损失是0.0428691990673542\n",
      "当前程序运行结束时间为：Thu Oct 13 12:13:40 2022\n",
      "当前正在训练第2900次， 损失是0.04286782443523407\n",
      "当前程序运行结束时间为：Thu Oct 13 12:15:00 2022\n",
      "当前正在训练第3000次， 损失是0.04285786300897598\n",
      "当前程序运行结束时间为：Thu Oct 13 12:16:19 2022\n",
      "当前正在训练第3100次， 损失是0.04290417954325676\n",
      "当前程序运行结束时间为：Thu Oct 13 12:17:38 2022\n",
      "当前正在训练第3200次， 损失是0.04281897842884064\n",
      "当前程序运行结束时间为：Thu Oct 13 12:18:55 2022\n",
      "当前正在训练第3300次， 损失是0.04291583225131035\n",
      "当前程序运行结束时间为：Thu Oct 13 12:20:13 2022\n",
      "当前正在训练第3400次， 损失是0.04284060373902321\n",
      "当前程序运行结束时间为：Thu Oct 13 12:21:33 2022\n",
      "当前正在训练第3500次， 损失是0.042813342064619064\n",
      "当前程序运行结束时间为：Thu Oct 13 12:22:51 2022\n",
      "当前正在训练第3600次， 损失是0.042957205325365067\n",
      "当前程序运行结束时间为：Thu Oct 13 12:24:09 2022\n",
      "当前正在训练第3700次， 损失是0.04276932775974274\n",
      "当前程序运行结束时间为：Thu Oct 13 12:25:29 2022\n",
      "当前正在训练第3800次， 损失是0.04278126731514931\n",
      "当前程序运行结束时间为：Thu Oct 13 12:26:47 2022\n",
      "当前正在训练第3900次， 损失是0.04279378801584244\n",
      "当前程序运行结束时间为：Thu Oct 13 12:28:05 2022\n",
      "当前正在训练第4000次， 损失是0.042766325175762177\n",
      "当前程序运行结束时间为：Thu Oct 13 12:29:23 2022\n",
      "当前正在训练第4100次， 损失是0.04289323836565018\n",
      "当前程序运行结束时间为：Thu Oct 13 12:30:40 2022\n",
      "当前正在训练第4200次， 损失是0.042762640863657\n",
      "当前程序运行结束时间为：Thu Oct 13 12:31:57 2022\n",
      "当前正在训练第4300次， 损失是0.042826034128665924\n",
      "当前程序运行结束时间为：Thu Oct 13 12:33:16 2022\n",
      "当前正在训练第4400次， 损失是0.042755380272865295\n",
      "当前程序运行结束时间为：Thu Oct 13 12:34:33 2022\n",
      "当前正在训练第4500次， 损失是0.042761895805597305\n",
      "当前程序运行结束时间为：Thu Oct 13 12:35:51 2022\n",
      "当前正在训练第4600次， 损失是0.042786382138729095\n",
      "当前程序运行结束时间为：Thu Oct 13 12:37:07 2022\n",
      "当前正在训练第4700次， 损失是0.04323751851916313\n",
      "当前程序运行结束时间为：Thu Oct 13 12:38:26 2022\n",
      "当前正在训练第4800次， 损失是0.042757797986269\n",
      "当前程序运行结束时间为：Thu Oct 13 12:39:42 2022\n",
      "当前正在训练第4900次， 损失是0.042775414884090424\n",
      "当前程序运行结束时间为：Thu Oct 13 12:40:58 2022\n",
      "当前正在训练第5000次， 损失是0.043254304677248\n",
      "当前程序运行结束时间为：Thu Oct 13 12:42:16 2022\n",
      "当前正在训练第5100次， 损失是0.042760055512189865\n",
      "当前程序运行结束时间为：Thu Oct 13 12:43:32 2022\n",
      "当前正在训练第5200次， 损失是0.042773161083459854\n",
      "当前程序运行结束时间为：Thu Oct 13 12:44:49 2022\n",
      "当前正在训练第5300次， 损失是0.042759835720062256\n",
      "当前程序运行结束时间为：Thu Oct 13 12:46:08 2022\n",
      "当前正在训练第5400次， 损失是0.04277133569121361\n",
      "当前程序运行结束时间为：Thu Oct 13 12:47:24 2022\n",
      "当前正在训练第5500次， 损失是0.0469544418156147\n",
      "当前程序运行结束时间为：Thu Oct 13 12:48:42 2022\n",
      "当前正在训练第5600次， 损失是0.04276487976312637\n",
      "当前程序运行结束时间为：Thu Oct 13 12:49:59 2022\n",
      "当前正在训练第5700次， 损失是0.0427866093814373\n",
      "当前程序运行结束时间为：Thu Oct 13 12:51:17 2022\n",
      "当前正在训练第5800次， 损失是0.042835671454668045\n",
      "当前程序运行结束时间为：Thu Oct 13 12:52:36 2022\n",
      "当前正在训练第5900次， 损失是0.042766377329826355\n",
      "当前程序运行结束时间为：Thu Oct 13 12:53:53 2022\n",
      "当前正在训练第6000次， 损失是0.04277157783508301\n",
      "当前程序运行结束时间为：Thu Oct 13 12:55:11 2022\n",
      "当前正在训练第6100次， 损失是0.042842309921979904\n",
      "当前程序运行结束时间为：Thu Oct 13 12:56:28 2022\n",
      "当前正在训练第6200次， 损失是0.04401717334985733\n",
      "当前程序运行结束时间为：Thu Oct 13 12:57:46 2022\n",
      "当前正在训练第6300次， 损失是0.04276524484157562\n",
      "当前程序运行结束时间为：Thu Oct 13 12:59:04 2022\n",
      "当前正在训练第6400次， 损失是0.04278004541993141\n",
      "当前程序运行结束时间为：Thu Oct 13 13:00:22 2022\n",
      "当前正在训练第6500次， 损失是0.047686733305454254\n",
      "当前程序运行结束时间为：Thu Oct 13 13:01:40 2022\n",
      "当前正在训练第6600次， 损失是0.04280288517475128\n",
      "当前程序运行结束时间为：Thu Oct 13 13:02:57 2022\n",
      "当前正在训练第6700次， 损失是0.04282373934984207\n",
      "当前程序运行结束时间为：Thu Oct 13 13:04:15 2022\n",
      "当前正在训练第6800次， 损失是0.04278828948736191\n",
      "当前程序运行结束时间为：Thu Oct 13 13:05:34 2022\n",
      "当前正在训练第6900次， 损失是0.04280342906713486\n",
      "当前程序运行结束时间为：Thu Oct 13 13:06:50 2022\n",
      "当前正在训练第7000次， 损失是0.04277821257710457\n",
      "当前程序运行结束时间为：Thu Oct 13 13:08:10 2022\n",
      "当前正在训练第7100次， 损失是0.042792633175849915\n",
      "当前程序运行结束时间为：Thu Oct 13 13:09:27 2022\n",
      "当前正在训练第7200次， 损失是0.042768802493810654\n",
      "当前程序运行结束时间为：Thu Oct 13 13:10:45 2022\n",
      "当前正在训练第7300次， 损失是0.04279305040836334\n",
      "当前程序运行结束时间为：Thu Oct 13 13:12:03 2022\n",
      "当前正在训练第7400次， 损失是0.042821284383535385\n",
      "当前程序运行结束时间为：Thu Oct 13 13:13:21 2022\n",
      "当前正在训练第7500次， 损失是0.04283292591571808\n",
      "当前程序运行结束时间为：Thu Oct 13 13:14:39 2022\n",
      "当前正在训练第7600次， 损失是0.042801178991794586\n",
      "当前程序运行结束时间为：Thu Oct 13 13:15:58 2022\n",
      "当前正在训练第7700次， 损失是0.042830683290958405\n",
      "当前程序运行结束时间为：Thu Oct 13 13:17:15 2022\n",
      "当前正在训练第7800次， 损失是0.0427866131067276\n",
      "当前程序运行结束时间为：Thu Oct 13 13:18:35 2022\n",
      "当前正在训练第7900次， 损失是0.0427844263613224\n",
      "当前程序运行结束时间为：Thu Oct 13 13:19:53 2022\n",
      "当前正在训练第8000次， 损失是0.042776256799697876\n",
      "当前程序运行结束时间为：Thu Oct 13 13:21:10 2022\n",
      "当前正在训练第8100次， 损失是0.04281758889555931\n",
      "当前程序运行结束时间为：Thu Oct 13 13:22:28 2022\n",
      "当前正在训练第8200次， 损失是0.043544165790081024\n",
      "当前程序运行结束时间为：Thu Oct 13 13:23:45 2022\n",
      "当前正在训练第8300次， 损失是0.04504663124680519\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m训练满足精度要求，训练了\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m次。\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i))\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrain_with_constrains\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [16], line 15\u001b[0m, in \u001b[0;36mtrain_with_constrains\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m (criterion((y_pred)\u001b[38;5;241m.\u001b[39mfloat(), (y_train)\u001b[38;5;241m.\u001b[39mfloat()) \u001b[38;5;241m/\u001b[39m y_train\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_layer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(constraints)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_with_constrains(): \n",
    "    model = CvxModel(n_feature=4, n_hidden=200, n_output=1)\n",
    "    constraints = weightConstraint()\n",
    "    criterion = torch.nn.MSELoss(reduction='sum')\n",
    "    learning_rate = 1e-1  #更改了学习率 loss下降的太低了\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    terminate_threshold = 1e-3 # 1e-5\n",
    "    i = 0\n",
    "    print(\"当前程序开始运行时间为：{}\".format(time.asctime(time.localtime(time.time()))))\n",
    "    while (True):\n",
    "        \n",
    "        y_pred = model(x_train)\n",
    "        loss = (criterion((y_pred).float(), (y_train).float()) / y_train.size()[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model._modules['input_layer'].apply(constraints)\n",
    "        model._modules['hidden_layer'].apply(constraints)\n",
    "        model._modules['output_layer'].apply(constraints)\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            print(\"当前程序运行结束时间为：{}\".format(time.asctime(time.localtime(time.time()))))\n",
    "            print(\"当前正在训练第{}次， 损失是{}\".format(i, loss.item()))\n",
    "#             print(y_train-y_pred)\n",
    "            # test_model(model)\n",
    "            # weight_print(model)\n",
    "        if loss.item() < terminate_threshold:\n",
    "            print(\"训练满足精度要求，训练了{}次。\".format(i))\n",
    "            break\n",
    "\n",
    "train_with_constrains()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bcc9a1",
   "metadata": {},
   "source": [
    "#### 当前正在训练第300次， 损失是0.047510839998722076"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9e8d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前程序开始运行时间为：Thu Oct 13 13:24:38 2022\n",
      "当前程序运行结束时间为：Thu Oct 13 13:28:55 2022\n",
      "当前正在训练第100次， 损失是0.3930050730705261\n",
      "当前程序运行结束时间为：Thu Oct 13 13:33:20 2022\n",
      "当前正在训练第200次， 损失是0.2473527491092682\n",
      "当前程序运行结束时间为：Thu Oct 13 13:38:01 2022\n",
      "当前正在训练第300次， 损失是0.16390392184257507\n",
      "当前程序运行结束时间为：Thu Oct 13 13:42:31 2022\n",
      "当前正在训练第400次， 损失是0.1134653389453888\n",
      "当前程序运行结束时间为：Thu Oct 13 13:46:45 2022\n",
      "当前正在训练第500次， 损失是0.08443906903266907\n",
      "当前程序运行结束时间为：Thu Oct 13 13:51:11 2022\n",
      "当前正在训练第600次， 损失是0.06781134009361267\n",
      "当前程序运行结束时间为：Thu Oct 13 13:55:27 2022\n",
      "当前正在训练第700次， 损失是0.0578727126121521\n",
      "当前程序运行结束时间为：Thu Oct 13 13:59:48 2022\n",
      "当前正在训练第800次， 损失是0.05192474648356438\n",
      "当前程序运行结束时间为：Thu Oct 13 14:04:10 2022\n",
      "当前正在训练第900次， 损失是0.04865332692861557\n",
      "当前程序运行结束时间为：Thu Oct 13 14:08:46 2022\n",
      "当前正在训练第1000次， 损失是0.04667331650853157\n",
      "当前程序运行结束时间为：Thu Oct 13 14:13:24 2022\n",
      "当前正在训练第1100次， 损失是0.04537602886557579\n",
      "当前程序运行结束时间为：Thu Oct 13 14:18:12 2022\n",
      "当前正在训练第1200次， 损失是0.044472016394138336\n",
      "当前程序运行结束时间为：Thu Oct 13 14:26:54 2022\n",
      "当前正在训练第1400次， 损失是0.04355146363377571\n",
      "当前程序运行结束时间为：Thu Oct 13 14:31:11 2022\n",
      "当前正在训练第1500次， 损失是0.04330490902066231\n",
      "当前程序运行结束时间为：Thu Oct 13 14:35:42 2022\n",
      "当前正在训练第1600次， 损失是0.043140385299921036\n",
      "当前程序运行结束时间为：Thu Oct 13 14:44:28 2022\n",
      "当前正在训练第1800次， 损失是0.0429515466094017\n",
      "当前程序运行结束时间为：Thu Oct 13 14:48:48 2022\n",
      "当前正在训练第1900次， 损失是0.042852312326431274\n",
      "当前程序运行结束时间为：Thu Oct 13 14:53:12 2022\n",
      "当前正在训练第2000次， 损失是0.04286612197756767\n",
      "当前程序运行结束时间为：Thu Oct 13 14:57:29 2022\n",
      "当前正在训练第2100次， 损失是0.042861923575401306\n",
      "当前程序运行结束时间为：Thu Oct 13 15:01:44 2022\n",
      "当前正在训练第2200次， 损失是0.0429367795586586\n",
      "当前程序运行结束时间为：Thu Oct 13 15:06:02 2022\n",
      "当前正在训练第2300次， 损失是0.042803891003131866\n",
      "当前程序运行结束时间为：Thu Oct 13 15:10:36 2022\n",
      "当前正在训练第2400次， 损失是0.0427231565117836\n",
      "当前程序运行结束时间为：Thu Oct 13 15:15:08 2022\n",
      "当前正在训练第2500次， 损失是0.04393230006098747\n",
      "当前程序运行结束时间为：Thu Oct 13 15:19:43 2022\n",
      "当前正在训练第2600次， 损失是0.0426047220826149\n",
      "当前程序运行结束时间为：Thu Oct 13 15:24:21 2022\n",
      "当前正在训练第2700次， 损失是0.04343786463141441\n",
      "当前程序运行结束时间为：Thu Oct 13 15:29:02 2022\n",
      "当前正在训练第2800次， 损失是0.044990424066782\n",
      "当前程序运行结束时间为：Thu Oct 13 15:33:28 2022\n",
      "当前正在训练第2900次， 损失是0.042537491768598557\n",
      "当前程序运行结束时间为：Thu Oct 13 15:38:17 2022\n",
      "当前正在训练第3000次， 损失是0.04253551363945007\n",
      "当前程序运行结束时间为：Thu Oct 13 15:42:54 2022\n",
      "当前正在训练第3100次， 损失是0.04273567348718643\n",
      "当前程序运行结束时间为：Thu Oct 13 15:47:28 2022\n",
      "当前正在训练第3200次， 损失是0.042539551854133606\n",
      "当前程序运行结束时间为：Thu Oct 13 15:52:02 2022\n",
      "当前正在训练第3300次， 损失是0.048355795443058014\n",
      "当前程序运行结束时间为：Thu Oct 13 15:56:20 2022\n",
      "当前正在训练第3400次， 损失是0.0464741587638855\n",
      "当前程序运行结束时间为：Thu Oct 13 16:00:48 2022\n",
      "当前正在训练第3500次， 损失是0.04303445294499397\n",
      "当前程序运行结束时间为：Thu Oct 13 16:05:24 2022\n",
      "当前正在训练第3600次， 损失是0.04270150884985924\n",
      "当前程序运行结束时间为：Thu Oct 13 16:10:05 2022\n",
      "当前正在训练第3700次， 损失是0.04271946847438812\n",
      "当前程序运行结束时间为：Thu Oct 13 16:14:53 2022\n",
      "当前正在训练第3800次， 损失是0.045515745878219604\n",
      "当前程序运行结束时间为：Thu Oct 13 16:19:27 2022\n",
      "当前正在训练第3900次， 损失是0.04762884974479675\n",
      "当前程序运行结束时间为：Thu Oct 13 16:24:03 2022\n",
      "当前正在训练第4000次， 损失是0.04256214573979378\n",
      "当前程序运行结束时间为：Thu Oct 13 16:28:25 2022\n",
      "当前正在训练第4100次， 损失是0.042473919689655304\n",
      "当前程序运行结束时间为：Thu Oct 13 16:32:41 2022\n",
      "当前正在训练第4200次， 损失是0.044798653572797775\n",
      "当前程序运行结束时间为：Thu Oct 13 16:37:12 2022\n",
      "当前正在训练第4300次， 损失是0.04264547675848007\n",
      "当前程序运行结束时间为：Thu Oct 13 16:41:35 2022\n",
      "当前正在训练第4400次， 损失是0.04256987199187279\n",
      "当前程序运行结束时间为：Thu Oct 13 16:45:59 2022\n",
      "当前正在训练第4500次， 损失是0.0426170714199543\n",
      "当前程序运行结束时间为：Thu Oct 13 16:50:54 2022\n",
      "当前正在训练第4600次， 损失是0.04320037364959717\n",
      "当前程序运行结束时间为：Thu Oct 13 16:55:41 2022\n",
      "当前正在训练第4700次， 损失是0.04311986267566681\n",
      "当前程序运行结束时间为：Thu Oct 13 17:00:17 2022\n",
      "当前正在训练第4800次， 损失是0.046184275299310684\n",
      "当前程序运行结束时间为：Thu Oct 13 17:04:50 2022\n",
      "当前正在训练第4900次， 损失是0.043718449771404266\n",
      "当前程序运行结束时间为：Thu Oct 13 17:09:26 2022\n",
      "当前正在训练第5000次， 损失是0.04330621287226677\n",
      "当前程序运行结束时间为：Thu Oct 13 17:13:55 2022\n",
      "当前正在训练第5100次， 损失是0.04250936582684517\n",
      "当前程序运行结束时间为：Thu Oct 13 17:18:33 2022\n",
      "当前正在训练第5200次， 损失是0.04258917272090912\n",
      "当前程序运行结束时间为：Thu Oct 13 17:23:09 2022\n",
      "当前正在训练第5300次， 损失是0.04243900254368782\n",
      "当前程序运行结束时间为：Thu Oct 13 17:27:40 2022\n",
      "当前正在训练第5400次， 损失是0.04245114326477051\n",
      "当前程序运行结束时间为：Thu Oct 13 17:32:03 2022\n",
      "当前正在训练第5500次， 损失是0.04252457618713379\n",
      "当前程序运行结束时间为：Thu Oct 13 17:36:29 2022\n",
      "当前正在训练第5600次， 损失是0.04501126706600189\n",
      "当前程序运行结束时间为：Thu Oct 13 17:41:11 2022\n",
      "当前正在训练第5700次， 损失是0.04260748252272606\n",
      "当前程序运行结束时间为：Thu Oct 13 17:45:54 2022\n",
      "当前正在训练第5800次， 损失是0.043290894478559494\n",
      "当前程序运行结束时间为：Thu Oct 13 17:50:46 2022\n",
      "当前正在训练第5900次， 损失是0.0436290018260479\n",
      "当前程序运行结束时间为：Thu Oct 13 17:55:22 2022\n",
      "当前正在训练第6000次， 损失是0.0425742082297802\n",
      "当前程序运行结束时间为：Thu Oct 13 17:59:52 2022\n",
      "当前正在训练第6100次， 损失是0.04375384747982025\n",
      "当前程序运行结束时间为：Thu Oct 13 18:04:31 2022\n",
      "当前正在训练第6200次， 损失是0.0426366962492466\n",
      "当前程序运行结束时间为：Thu Oct 13 18:09:09 2022\n",
      "当前正在训练第6300次， 损失是0.04243550822138786\n",
      "当前程序运行结束时间为：Thu Oct 13 18:13:45 2022\n",
      "当前正在训练第6400次， 损失是0.043706607073545456\n",
      "当前程序运行结束时间为：Thu Oct 13 18:18:18 2022\n",
      "当前正在训练第6500次， 损失是0.04279063642024994\n",
      "当前程序运行结束时间为：Thu Oct 13 18:22:49 2022\n",
      "当前正在训练第6600次， 损失是0.042845264077186584\n",
      "当前程序运行结束时间为：Thu Oct 13 18:27:23 2022\n",
      "当前正在训练第6700次， 损失是0.04244029149413109\n",
      "当前程序运行结束时间为：Thu Oct 13 18:32:05 2022\n",
      "当前正在训练第6800次， 损失是0.04307928681373596\n",
      "当前程序运行结束时间为：Thu Oct 13 18:36:42 2022\n",
      "当前正在训练第6900次， 损失是0.042778920382261276\n",
      "当前程序运行结束时间为：Thu Oct 13 18:41:08 2022\n",
      "当前正在训练第7000次， 损失是0.04244513437151909\n",
      "当前程序运行结束时间为：Thu Oct 13 18:45:44 2022\n",
      "当前正在训练第7100次， 损失是0.045013412833213806\n",
      "当前程序运行结束时间为：Thu Oct 13 18:50:43 2022\n",
      "当前正在训练第7200次， 损失是0.04338669404387474\n",
      "当前程序运行结束时间为：Thu Oct 13 18:55:28 2022\n",
      "当前正在训练第7300次， 损失是0.042404431849718094\n",
      "当前程序运行结束时间为：Thu Oct 13 19:00:17 2022\n",
      "当前正在训练第7400次， 损失是0.04272083565592766\n",
      "当前程序运行结束时间为：Thu Oct 13 19:04:53 2022\n",
      "当前正在训练第7500次， 损失是0.04244038835167885\n",
      "当前程序运行结束时间为：Thu Oct 13 19:09:32 2022\n",
      "当前正在训练第7600次， 损失是0.04304150119423866\n",
      "当前程序运行结束时间为：Thu Oct 13 19:14:20 2022\n",
      "当前正在训练第7700次， 损失是0.04240242391824722\n",
      "当前程序运行结束时间为：Thu Oct 13 19:18:54 2022\n",
      "当前正在训练第7800次， 损失是0.04246492311358452\n",
      "当前程序运行结束时间为：Thu Oct 13 19:23:38 2022\n",
      "当前正在训练第7900次， 损失是0.04248819127678871\n",
      "当前程序运行结束时间为：Thu Oct 13 19:28:29 2022\n",
      "当前正在训练第8000次， 损失是0.04562436416745186\n",
      "当前程序运行结束时间为：Thu Oct 13 19:33:12 2022\n",
      "当前正在训练第8100次， 损失是0.04276322200894356\n",
      "当前程序运行结束时间为：Thu Oct 13 19:37:36 2022\n",
      "当前正在训练第8200次， 损失是0.042480096220970154\n",
      "当前程序运行结束时间为：Thu Oct 13 19:41:58 2022\n",
      "当前正在训练第8300次， 损失是0.042448826134204865\n",
      "当前程序运行结束时间为：Thu Oct 13 19:46:24 2022\n",
      "当前正在训练第8400次， 损失是0.042703717947006226\n",
      "当前程序运行结束时间为：Thu Oct 13 19:51:07 2022\n",
      "当前正在训练第8500次， 损失是0.042586956173181534\n",
      "当前程序运行结束时间为：Thu Oct 13 19:55:37 2022\n",
      "当前正在训练第8600次， 损失是0.04325413331389427\n",
      "当前程序运行结束时间为：Thu Oct 13 20:00:00 2022\n",
      "当前正在训练第8700次， 损失是0.042692672461271286\n",
      "当前程序运行结束时间为：Thu Oct 13 20:04:29 2022\n",
      "当前正在训练第8800次， 损失是0.04283915460109711\n",
      "当前程序运行结束时间为：Thu Oct 13 20:08:59 2022\n",
      "当前正在训练第8900次， 损失是0.04265323281288147\n",
      "当前程序运行结束时间为：Thu Oct 13 20:13:17 2022\n",
      "当前正在训练第9000次， 损失是0.04240107536315918\n",
      "当前程序运行结束时间为：Thu Oct 13 20:17:43 2022\n",
      "当前正在训练第9100次， 损失是0.04239062964916229\n",
      "当前程序运行结束时间为：Thu Oct 13 20:22:16 2022\n",
      "当前正在训练第9200次， 损失是0.042773764580488205\n",
      "当前程序运行结束时间为：Thu Oct 13 20:26:39 2022\n",
      "当前正在训练第9300次， 损失是0.04276624321937561\n"
     ]
    }
   ],
   "source": [
    "class CvxModel(nn.Module):\n",
    "    '''\n",
    "        输入凸神经网络结构定义\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_feature=4, n_hidden=4, n_output=1, nGdIter=30):\n",
    "        super(CvxModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(n_feature, n_hidden, bias=False)\n",
    "        self.hidden_layer = nn.Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hidden_layer = nn.Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.output_layer = nn.Linear(n_hidden, n_output, bias=False)\n",
    "        self.passthrough_layer = nn.Linear(n_feature, n_hidden)\n",
    "        self.passthrough_output_layer = nn.Linear(n_feature, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        zx1 = F.relu(self.input_layer(x))\n",
    "        pass1 = self.passthrough_layer(x)\n",
    "        pass2 = self.passthrough_output_layer(x)\n",
    "        zx2 = F.relu(self.hidden_layer(zx1) + pass1)\n",
    "        zx3 = self.output_layer(zx2) + pass2\n",
    "        zx3 = zx3.squeeze(-1)\n",
    "        return zx3\n",
    "改变p高\n",
    "\n",
    "class weightConstraint():\n",
    "    '''\n",
    "        对一个特定的层的weight进行参数限制\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, module):\n",
    "        if hasattr(module, 'weight'):\n",
    "            w = module.weight.data\n",
    "            w = w.clamp(0.0, 100.0) #clamp函数可以设置最大最小值，\n",
    "            module.weight.data = w\n",
    "\n",
    "def train_with_constrains(): \n",
    "    model = CvxModel(n_feature=4, n_hidden=500, n_output=1)\n",
    "    constraints = weightConstraint()\n",
    "    criterion = torch.nn.MSELoss(reduction='sum')\n",
    "    learning_rate = 1e-1  #更改了学习率 loss下降的太低了\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    terminate_threshold = 3e-3 # 1e-5\n",
    "    i = 0\n",
    "    print(\"当前程序开始运行时间为：{}\".format(time.asctime(time.localtime(time.time()))))\n",
    "    while (True):\n",
    "        \n",
    "        y_pred = model(x_train)\n",
    "        loss = (criterion((y_pred).float(), (y_train).float()) / y_train.size()[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model._modules['input_layer'].apply(constraints)\n",
    "        model._modules['hidden_layer'].apply(constraints)\n",
    "        model._modules['output_layer'].apply(constraints)\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            print(\"当前程序运行结束时间为：{}\".format(time.asctime(time.localtime(time.time()))))\n",
    "            print(\"当前正在训练第{}次， 损失是{}\".format(i, loss.item()))\n",
    "#             print(y_train-y_pred)\n",
    "            # test_model(model)\n",
    "            # weight_print(model)\n",
    "        if loss.item() < terminate_threshold:\n",
    "            print(\"训练满足精度要求，训练了{}次。\".format(i))\n",
    "            break\n",
    "\n",
    "train_with_constrains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ad455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
